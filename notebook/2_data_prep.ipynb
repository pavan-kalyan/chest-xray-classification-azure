{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Data Prep\n",
    "**This notebook will generate the pickle files which contain the info about the train/val/test split and store it in data/processed/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Allow multiple displays per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up all the paths\n",
    "base_path = os.path.dirname(os.getcwd())\n",
    "data_raw_dir = os.path.join(base_path,'data/raw')\n",
    "data_proc_dir = os.path.join(base_path,'data/processed')\n",
    "data_img_dir = os.path.join(base_path,'data/processed/images')\n",
    "label_file = os.path.join(data_raw_dir,'Data_Entry_2017.csv')\n",
    "notshared_dir = os.path.join(base_path,'notshared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_df = pd.read_csv(label_file)\n",
    "\n",
    "\n",
    "total_patient_number = labels_df['Patient ID'].nunique()\n",
    "NIH_patients_and_labels_file = 'Data_Entry_2017.csv'\n",
    "NIH_annotated_file = 'BBox_List_2017.csv' # exclude from train pathology annotated by radiologists \n",
    "bad_imgs_file = 'blacklist.txt'# exclude what viusally looks like bad images\n",
    "\n",
    "patient_id_original = [i for i in range(1,total_patient_number + 1)]\n",
    "\n",
    "# ignored images list is used later, since this is not a patient ID level issue\n",
    "\n",
    "ignored_imgs_ids = []\n",
    "bad_imgs = open(os.path.join(data_raw_dir,'blacklist.txt'))\n",
    "ignored_imgs_ids = bad_imgs.read().splitlines()\n",
    "bad_imgs.close()\n",
    "\n",
    "bbox_df = pd.read_csv(os.path.join(data_raw_dir,NIH_annotated_file))\n",
    "bbox_patient_index_df = bbox_df['Image Index'].str.slice(3, 8)\n",
    "\n",
    "bbox_patient_index_list = []\n",
    "for index, item in bbox_patient_index_df.iteritems():\n",
    "    bbox_patient_index_list.append(int(item))\n",
    "\n",
    "patient_id = list(set(patient_id_original) - set(bbox_patient_index_list))\n",
    "print(\"len of original patient id is\", len(patient_id_original))\n",
    "print(\"len of cleaned patient id is\", len(patient_id))\n",
    "print(\"len of unique patient id with annotated data\", \n",
    "      len(list(set(bbox_patient_index_list))))\n",
    "print(\"len of patient id with annotated data\",bbox_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "random.shuffle(patient_id)\n",
    "\n",
    "print(\"first ten patient ids are\", patient_id[:10])\n",
    "\n",
    "# training:valid:test=7:1:2\n",
    "patient_id_train = patient_id[:int(total_patient_number * 0.7)]\n",
    "patient_id_valid = patient_id[int(total_patient_number * 0.7):int(total_patient_number * 0.8)]\n",
    "# get the rest of the patient_id as the test set\n",
    "patient_id_test = patient_id[int(total_patient_number * 0.8):]\n",
    "patient_id_test.extend(bbox_patient_index_list)\n",
    "patient_id_test = list(set(patient_id_test))\n",
    "\n",
    "print(\"train:{} valid:{} test:{}\".format(len(patient_id_train), len(patient_id_valid), len(patient_id_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.tail()\n",
    "labels_df['Finding Labels'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(len(labels_df['Image Index']))\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = labels_df['Finding Labels'].str.split( '|', expand=False).str.join(sep='*').str.get_dummies(sep='*')\n",
    "x_axis = list(dummy.columns)\n",
    "y_axis = list(dummy.sum())\n",
    "plt.title('distribution of diseases in dataset');\n",
    "plt.xticks(list(range(1,16)),x_axis,rotation='vertical')\n",
    "plt.bar(list(range(1,16)),y_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASE_LIST = x_axis\n",
    "DISEASE_LIST.remove('No Finding')\n",
    "for id,ele in enumerate(DISEASE_LIST):\n",
    "    print(str(id) + ' - '+ ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels_and_indices(df, patient_ids,ignored_ids):\n",
    "    img_name_ids = []\n",
    "    img_labels = {}\n",
    "    for patient in tqdm.tqdm(patient_ids):\n",
    "        for _, row in df[df['Patient ID'] == patient].iterrows():\n",
    "            img_idx = row['Image Index']\n",
    "            if img_idx not in ignored_ids:\n",
    "                img_name_ids.append(img_idx)\n",
    "                img_labels[img_idx] = np.zeros(14, dtype=np.uint8)\n",
    "                for disease_idx,disease_name in enumerate(DISEASE_LIST):\n",
    "                    if disease_name in row['Finding Labels'].split('|'):\n",
    "                        img_labels[img_idx][disease_idx] = 1\n",
    "                    else:\n",
    "                        img_labels[img_idx][disease_idx] = 0\n",
    "    return img_name_ids, img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_index, train_labels = generate_labels_and_indices(labels_df, patient_id_train,ignored_imgs_ids)\n",
    "valid_data_index, valid_labels = generate_labels_and_indices(labels_df, patient_id_valid,ignored_imgs_ids)\n",
    "test_data_index, test_labels = generate_labels_and_indices(labels_df, patient_id_test,ignored_imgs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: train_labels[k] for k in list(train_labels)[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking one sample\n",
    "labels_df[labels_df['Image Index'] == '00000024_000.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train, valid, test image number is:\", len(train_data_index), len(valid_data_index), len(test_data_index))\n",
    "\n",
    "# save the data\n",
    "labels_all = {}\n",
    "labels_all.update(train_labels)\n",
    "labels_all.update(valid_labels)\n",
    "labels_all.update(test_labels)\n",
    "\n",
    "partition_dict = {'train': train_data_index, 'test': test_data_index, 'valid': valid_data_index}\n",
    "\n",
    "with open(os.path.join(data_proc_dir,'labels14_unormalized_cleaned.pickle'), 'wb') as f:\n",
    "    pickle.dump(labels_all, f)\n",
    "\n",
    "with open(os.path.join(data_proc_dir,'partition14_unormalized_cleaned.pickle'), 'wb') as f:\n",
    "    pickle.dump(partition_dict, f)\n",
    "    \n",
    "# also save the patient id partitions for pytorch training    \n",
    "with open(os.path.join(data_proc_dir,'train_test_valid_data_partitions.pickle'), 'wb') as f:\n",
    "    pickle.dump([patient_id_train,patient_id_valid,\n",
    "                 patient_id_test,\n",
    "                list(set(bbox_patient_index_list))], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/processed/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell will generate the python script file and store it in src/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/1_data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/1_data_prep.py\n",
    "\n",
    "import azureml.core\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "#setting up all the paths\n",
    "base_path = os.path.dirname(os.getcwd())\n",
    "data_raw_dir = os.path.join(base_path,'data/raw')\n",
    "data_proc_dir = os.path.join(base_path,'data/processed')\n",
    "data_img_dir = os.path.join(base_path,'data/processed/images')\n",
    "label_file = os.path.join(data_raw_dir,'Data_Entry_2017.csv')\n",
    "notshared_dir = os.path.join(base_path,'notshared')\n",
    "\n",
    "labels_df = pd.read_csv(label_file)\n",
    "\n",
    "\n",
    "total_patient_number = labels_df['Patient ID'].nunique()\n",
    "NIH_patients_and_labels_file = 'Data_Entry_2017.csv'\n",
    "NIH_annotated_file = 'BBox_List_2017.csv' # exclude from train pathology annotated by radiologists \n",
    "bad_imgs_file = 'blacklist.txt'# exclude what viusally looks like bad images\n",
    "\n",
    "patient_id_original = [i for i in range(1,total_patient_number + 1)]\n",
    "\n",
    "# ignored images list is used later, since this is not a patient ID level issue\n",
    "\n",
    "ignored_imgs_ids = []\n",
    "bad_imgs = open(os.path.join(data_raw_dir,'blacklist.txt'))\n",
    "ignored_imgs_ids = bad_imgs.read().splitlines()\n",
    "bad_imgs.close()\n",
    "\n",
    "bbox_df = pd.read_csv(os.path.join(data_raw_dir,NIH_annotated_file))\n",
    "bbox_patient_index_df = bbox_df['Image Index'].str.slice(3, 8)\n",
    "\n",
    "bbox_patient_index_list = []\n",
    "for index, item in bbox_patient_index_df.iteritems():\n",
    "    bbox_patient_index_list.append(int(item))\n",
    "\n",
    "patient_id = list(set(patient_id_original) - set(bbox_patient_index_list))\n",
    "print(\"len of original patient id is\", len(patient_id_original))\n",
    "print(\"len of cleaned patient id is\", len(patient_id))\n",
    "print(\"len of unique patient id with annotated data\", \n",
    "      len(list(set(bbox_patient_index_list))))\n",
    "print(\"len of patient id with annotated data\",bbox_df.shape[0])\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(patient_id)\n",
    "\n",
    "print(\"first ten patient ids are\", patient_id[:10])\n",
    "\n",
    "# training:valid:test=7:1:2\n",
    "patient_id_train = patient_id[:int(total_patient_number * 0.7)]\n",
    "patient_id_valid = patient_id[int(total_patient_number * 0.7):int(total_patient_number * 0.8)]\n",
    "# get the rest of the patient_id as the test set\n",
    "patient_id_test = patient_id[int(total_patient_number * 0.8):]\n",
    "patient_id_test.extend(bbox_patient_index_list)\n",
    "patient_id_test = list(set(patient_id_test))\n",
    "\n",
    "print(\"train:{} valid:{} test:{}\".format(len(patient_id_train), len(patient_id_valid), len(patient_id_test)))\n",
    "\n",
    "labels_df.tail()\n",
    "labels_df['Finding Labels'].nunique()\n",
    "\n",
    "index = np.arange(len(labels_df['Image Index']))\n",
    "print(index)\n",
    "\n",
    "dummy = labels_df['Finding Labels'].str.split( '|', expand=False).str.join(sep='*').str.get_dummies(sep='*')\n",
    "x_axis = list(dummy.columns)\n",
    "y_axis = list(dummy.sum())\n",
    "plt.title('distribution of diseases in dataset');\n",
    "plt.xticks(list(range(1,16)),x_axis,rotation='vertical')\n",
    "plt.bar(list(range(1,16)),y_axis)\n",
    "\n",
    "DISEASE_LIST = x_axis\n",
    "DISEASE_LIST.remove('No Finding')\n",
    "for id,ele in enumerate(DISEASE_LIST):\n",
    "    print(str(id) + ' - '+ ele)\n",
    "    \n",
    "    \n",
    "def generate_labels_and_indices(df, patient_ids,ignored_ids):\n",
    "    img_name_ids = []\n",
    "    img_labels = {}\n",
    "    for patient in tqdm.tqdm(patient_ids):\n",
    "        for _, row in df[df['Patient ID'] == patient].iterrows():\n",
    "            img_idx = row['Image Index']\n",
    "            if img_idx not in ignored_ids:\n",
    "                img_name_ids.append(img_idx)\n",
    "                img_labels[img_idx] = np.zeros(14, dtype=np.uint8)\n",
    "                for disease_idx,disease_name in enumerate(DISEASE_LIST):\n",
    "                    if disease_name in row['Finding Labels'].split('|'):\n",
    "                        img_labels[img_idx][disease_idx] = 1\n",
    "                    else:\n",
    "                        img_labels[img_idx][disease_idx] = 0\n",
    "    return img_name_ids, img_labels\n",
    "\n",
    "train_data_index, train_labels = generate_labels_and_indices(labels_df, patient_id_train,ignored_imgs_ids)\n",
    "valid_data_index, valid_labels = generate_labels_and_indices(labels_df, patient_id_valid,ignored_imgs_ids)\n",
    "test_data_index, test_labels = generate_labels_and_indices(labels_df, patient_id_test,ignored_imgs_ids)\n",
    "type(train_labels)\n",
    "\n",
    "{k: train_labels[k] for k in list(train_labels)[:5]}\n",
    "\n",
    "#checking one sample\n",
    "labels_df[labels_df['Image Index'] == '00000024_000.png']\n",
    "print(\"train, valid, test image number is:\", len(train_data_index), len(valid_data_index), len(test_data_index))\n",
    "\n",
    "# save the data\n",
    "labels_all = {}\n",
    "labels_all.update(train_labels)\n",
    "labels_all.update(valid_labels)\n",
    "labels_all.update(test_labels)\n",
    "\n",
    "partition_dict = {'train': train_data_index, 'test': test_data_index, 'valid': valid_data_index}\n",
    "\n",
    "with open(os.path.join(data_proc_dir,'labels14_unormalized_cleaned.pickle'), 'wb') as f:\n",
    "    pickle.dump(labels_all, f)\n",
    "\n",
    "with open(os.path.join(data_proc_dir,'partition14_unormalized_cleaned.pickle'), 'wb') as f:\n",
    "    pickle.dump(partition_dict, f)\n",
    "    \n",
    "# also save the patient id partitions for pytorch training    \n",
    "with open(os.path.join(data_proc_dir,'train_test_valid_data_partitions.pickle'), 'wb') as f:\n",
    "    pickle.dump([patient_id_train,patient_id_valid,\n",
    "                 patient_id_test,\n",
    "                list(set(bbox_patient_index_list))], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
