{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook generates the python script and configures the compute target for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls\n",
    "!pwd\n",
    "import azureml.core\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import azureml.data\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core import Workspace, Datastore\n",
    "from azureml.core import Run\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import time\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import json\n",
    "import argparse\n",
    "base_path = os.path.dirname(os.getcwd())\n",
    "notshared_dir = os.path.join(base_path,'notshared')\n",
    "ws = Workspace.from_config('../notshared/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/pytorch_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/pytorch_train.py\n",
    "\n",
    "import azureml.core\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import azureml.data\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core import Workspace, Datastore\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import time\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import json\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import pretrainedmodels\n",
    "\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "print(azureml.core.VERSION)\n",
    "#print(ws.name)\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "args = parser.parse_args()\n",
    "print(args.data_folder)\n",
    "data_img_dir = args.data_folder\n",
    "print('Data folder:', data_img_dir)\n",
    "\n",
    "base_path = os.path.dirname(os.getcwd())\n",
    "label_file = 'Data_Entry_2017.csv'\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "import pickle\n",
    "patient_id_partition_file = 'train_test_valid_data_partitions.pickle'\n",
    "\n",
    "with open(patient_id_partition_file, 'rb') as f:\n",
    "    [train_set,valid_set,test_set, nih_annotated_set]=pickle.load(f)\n",
    "\n",
    "print(\"train:{} valid:{} test:{} nih-annotated:{}\".format(len(train_set), len(valid_set), \\\n",
    "                                                     len(test_set), len(nih_annotated_set)))\n",
    "\n",
    "# Globals\n",
    "# With small batch may be faster on P100 to do one 1 GPU\n",
    "MULTI_GPU = True\n",
    "CLASSES = 14\n",
    "WIDTH = 331\n",
    "HEIGHT = 331\n",
    "CHANNELS = 3\n",
    "LR = 0.0001\n",
    "EPOCHS = 10 #100\n",
    "# Can scale to max for inference but for training LR will be affected\n",
    "# Prob better to increase this though on P100 since LR is not too low\n",
    "# Easier to see when plotted\n",
    "BATCHSIZE = 64 #64*2\n",
    "IMAGENET_RGB_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_RGB_SD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class XrayData(Dataset):\n",
    "    def __init__(self, img_dir, lbl_file, patient_ids, transform=None):\n",
    "        \n",
    "        # Read labels-csv\n",
    "        df = pd.read_csv(lbl_file)\n",
    "        \n",
    "        df_label = df['Finding Labels'].str.split(\n",
    "            '|', expand=False).str.join(sep='*').str.get_dummies(sep='*')\n",
    "        if 'No Finding' in df_label.columns:\n",
    "            df_label.drop(['No Finding'], axis=1, inplace=True)\n",
    "        # Filter by patient-ids\n",
    "        self.labels = df_label.values[df['Patient ID'].isin(patient_ids)]\n",
    "        df = df[df['Patient ID'].isin(patient_ids)]\n",
    "        # Split labels\n",
    "        \n",
    "                \n",
    "        # List of images (full-path)\n",
    "        self.img_locs =  df['Image Index'].map(lambda im: os.path.join(img_dir, im)).values\n",
    "        # One-hot encoded labels (float32 for BCE loss)\n",
    "        \n",
    "        # Processing\n",
    "        self.transform = transform\n",
    "        print(\"Loaded {} labels and {} images\".format(len(self.labels), \n",
    "                                                      len(self.img_locs)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        im_file = self.img_locs[idx]\n",
    "        im_rgb = Image.open(im_file).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform is not None:\n",
    "            im_rgb = self.transform(im_rgb)\n",
    "        return im_rgb, torch.FloatTensor(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_locs)\n",
    "\n",
    "def no_augmentation_dataset(img_dir, lbl_file, patient_ids, normalize):\n",
    "    dataset = XrayData(img_dir, lbl_file, patient_ids,\n",
    "                       transform=transforms.Compose([\n",
    "                           #transforms.Resize(331),\n",
    "                           transforms.Resize((331,331),interpolation=Image.NEAREST),\n",
    "                           transforms.ToTensor(),  \n",
    "                           normalize]))\n",
    "    return dataset\n",
    "\n",
    "normalize = transforms.Normalize(IMAGENET_RGB_MEAN, IMAGENET_RGB_SD)\n",
    "\n",
    "# the following transformations will help generalize the model and prevent overfitting.\n",
    "train_dataset = XrayData(img_dir=data_img_dir,\n",
    "                         lbl_file=label_file,\n",
    "                         patient_ids=train_set,\n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.Resize(350),\n",
    "                             transforms.RandomHorizontalFlip(),\n",
    "                             transforms.RandomResizedCrop(size=WIDTH),\n",
    "                             transforms.ColorJitter(0.15, 0.15),\n",
    "                             transforms.RandomRotation(15),\n",
    "                             transforms.ToTensor(),  \n",
    "                             normalize]))\n",
    "\n",
    "\n",
    "\n",
    "valid_dataset = no_augmentation_dataset(data_img_dir, label_file, valid_set, normalize)\n",
    "test_dataset = no_augmentation_dataset(data_img_dir, label_file, test_set, normalize)\n",
    "\n",
    "model_name = 'nasnetalarge'\n",
    "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_classes=14\n",
    "\n",
    "in_ftrs=model.last_linear.in_features\n",
    "model.last_linear=nn.Sequential(\n",
    "nn.Linear(in_ftrs,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(256,n_classes),\n",
    "nn.Sigmoid())\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "else:\n",
    "    model.cuda()\n",
    "import pretrainedmodels.utils as utils\n",
    "\n",
    "def init_symbol(sym, lr=LR):\n",
    "    # torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    opt = optim.Adam(sym.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = ReduceLROnPlateau(opt, factor = 0.1, patience = 5, mode = 'min')\n",
    "    return opt, criterion, scheduler\n",
    "\n",
    "def compute_roc_auc(data_gt, data_pd, mean=True, classes=CLASSES):\n",
    "    roc_auc = []\n",
    "    data_gt = data_gt.cpu().numpy()\n",
    "    data_pd = data_pd.cpu().numpy()\n",
    "    for i in range(classes):\n",
    "        try:\n",
    "            roc_auc.append(roc_auc_score(data_gt[:, i], data_pd[:, i]))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    if mean:\n",
    "        roc_auc = np.mean(roc_auc)\n",
    "    return roc_auc\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, epoch, batch=BATCHSIZE):\n",
    "    model.train()\n",
    "    print(\"Training epoch {}\".format(epoch+1))\n",
    "    loss_val = 0\n",
    "    loss_cnt = 0\n",
    "    batch_count = 0\n",
    "    for data, target in dataloader:\n",
    "        # Get samples\n",
    "        batch_count = batch_count + 1\n",
    "        #print(batch_count)\n",
    "        data = torch.FloatTensor(data).cuda()\n",
    "        target = torch.FloatTensor(target).cuda()\n",
    "        # Init\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Forwards\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Back-prop\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "         # Log the loss\n",
    "        loss_val += loss.data.item()\n",
    "        loss_cnt += 1\n",
    "    print(\"Training loss: {0:.4f}\".format(loss_val/loss_cnt))\n",
    "    \n",
    "@torch.no_grad()\n",
    "def valid_epoch(model, dataloader, criterion, epoch, phase='valid', batch=BATCHSIZE):\n",
    "    model.eval()\n",
    "    if phase == 'testing':\n",
    "        print(\"Testing epoch {}\".format(epoch+1))\n",
    "    else:\n",
    "        print(\"Validating epoch {}\".format(epoch+1))\n",
    "    out_pred = torch.FloatTensor().cuda()\n",
    "    out_gt = torch.FloatTensor().cuda()\n",
    "    loss_val = 0\n",
    "    loss_cnt = 0\n",
    "    batch_count = 0\n",
    "    for data, target in dataloader:\n",
    "        # Get samples\n",
    "        batch_count = batch_count + 1\n",
    "        #print(batch_count)\n",
    "        data = torch.FloatTensor(data).cuda()\n",
    "        target = torch.FloatTensor(target).cuda()\n",
    "         # Forwards\n",
    "        output = model(data)\n",
    "        # Loss\n",
    "        loss = criterion(output, target)\n",
    "        # Log the loss\n",
    "        loss_val += loss.data.item()\n",
    "        loss_cnt += 1\n",
    "        # Log for AUC\n",
    "        out_pred = torch.cat((out_pred, output.data), 0)\n",
    "        out_gt = torch.cat((out_gt, target.data), 0)\n",
    "    loss_mean = loss_val/loss_cnt\n",
    "    if phase == 'testing':\n",
    "        print(\"Test-Dataset loss: {0:.4f}\".format(loss_mean))\n",
    "        print(\"Test-Dataset AUC: {0:.4f}\".format(compute_roc_auc(out_gt, out_pred)))\n",
    "\n",
    "    else:\n",
    "        print(\"Validation loss: {0:.4f}\".format(loss_mean))\n",
    "        print(\"Validation AUC: {0:.4f}\".format(compute_roc_auc(out_gt, out_pred)))\n",
    "    return loss_mean\n",
    "\n",
    "def print_learning_rate(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        print(\"Learining rate: \", param_group['lr'])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCHSIZE,\n",
    "                          shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=2*BATCHSIZE,\n",
    "                          shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=2*BATCHSIZE,\n",
    "                         shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "\n",
    "# Load optimiser, loss\n",
    "optimizer, criterion, scheduler = init_symbol(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    valid_epoch(model, valid_loader, criterion, -1)\n",
    "\n",
    "loss_min = float(\"inf\")    \n",
    "stime = time.time()\n",
    "# Main train/val/test loop\n",
    "for j in range(EPOCHS):\n",
    "    train_epoch(model, train_loader, optimizer, criterion, j)\n",
    "    print(\"after train\")\n",
    "    with torch.no_grad():\n",
    "        loss_val = valid_epoch(model, valid_loader, criterion, j)\n",
    "        test_loss_val = valid_epoch(model, test_loader, criterion, j, 'testing')\n",
    "    # LR Schedule\n",
    "    scheduler.step(loss_val)\n",
    "    print_learning_rate(optimizer)\n",
    "    \n",
    "    if loss_val < loss_min:\n",
    "        print(\"Loss decreased. Saving ...\")\n",
    "        loss_min = loss_val\n",
    "        torch.save({'epoch': j + 1, \n",
    "                    'state_dict': model.state_dict(), \n",
    "                    'best_loss': loss_min, \n",
    "                    'optimizer' : optimizer.state_dict()}, 'best_chexray_nasnet.pth.tar')\n",
    "    etime = time.time()\n",
    "    print(\"Epoch time: {0:.0f} seconds\".format(etime-stime))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    \n",
    "    \n",
    "run.log('loss', loss_min)\n",
    "\n",
    "\n",
    "\n",
    "# Load model for testing\n",
    "azure_chexray_sym_test = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_classes=14\n",
    "\n",
    "in_ftrs=azure_chexray_sym_test.last_linear.in_features\n",
    "azure_chexray_sym_test.last_linear=nn.Sequential(\n",
    "nn.Linear(in_ftrs,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.4),\n",
    "    nn.Linear(256,n_classes),\n",
    "nn.Sigmoid())\n",
    "azure_chexray_sym_test.eval()\n",
    "optimizer, criterion, scheduler = init_symbol(azure_chexray_sym_test)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs! for testing\")\n",
    "    azure_chexray_sym_test = nn.DataParallel(azure_chexray_sym_test).cuda()\n",
    "else:\n",
    "    azure_chexray_sym_test.cuda()\n",
    "chkpt = torch.load(\"best_chexray_nasnet.pth.tar\")\n",
    "azure_chexray_sym_test.load_state_dict(chkpt['state_dict'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    valid_loss = valid_epoch(azure_chexray_sym_test, valid_loader, criterion, -1)\n",
    "    test_loss = valid_epoch(azure_chexray_sym_test, test_loader, criterion, -1, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an experiment\n",
    "experiment_name = 'chexray-pytorch-nasnet'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = os.path.join(base_path,'amlcompute')\n",
    "os.makedirs(project_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/t-padama/Documents/GSMO_Internship/python ML Project/amlcompute/config.json'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#copying all the required files to the amlcompute folder for the compute target.\n",
    "\n",
    "import shutil\n",
    "\n",
    "shutil.copy('../src/pytorch_train.py', project_folder)\n",
    "shutil.copy('../data/processed/train_test_valid_data_partitions.pickle', project_folder)\n",
    "shutil.copy('../data/raw/Data_Entry_2017.csv',project_folder)\n",
    "shutil.copy('../notshared/config.json',project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chestds'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get azure blob storage and upload entire dataset to images folder for the compute target to access.\n",
    "\n",
    "with open(os.path.join(notshared_dir,'credentials.json')) as creds:    \n",
    "    credentials = json.load(creds)\n",
    "fds = Datastore.register_azure_file_share(workspace=ws, \n",
    "                                         datastore_name=credentials['datastore_name'], \n",
    "                                         file_share_name=credentials['file_share_name'],\n",
    "                                         account_name=credentials['account_name'], \n",
    "                                         account_key=credentials['account_key'],\n",
    "                                         create_if_not_exists=False)\n",
    "ws.set_default_datastore('chestds')\n",
    "fds.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspacefilestore AzureFile\n",
      "workspaceblobstore AzureBlob\n",
      "chexrayds AzureBlob\n",
      "chestds AzureFile\n"
     ]
    }
   ],
   "source": [
    "datastores = ws.datastores\n",
    "for name, ds in datastores.items():\n",
    "    print(name, ds.datastore_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.data\n",
    "from azureml.data.azure_storage_datastore import AzureFileDatastore, AzureBlobDatastore\n",
    "\n",
    "#uploading the entire dataset to the file share.\n",
    "fds.upload(src_dir=os.path.join(base_path,'data/processed/images'),\n",
    "          target_path='images',\n",
    "          overwrite=False,\n",
    "          show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cd = CondaDependencies(conda_dependencies_file_path=os.path.join(base_path,'env.yml'))\n",
    "print(list(cd.conda_packages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "#creating or attaching compute targets\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "gpu_cluster_name = \"gpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC12',\n",
    "                                                           max_nodes=4)\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "\n",
    "gpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<azureml.core.runconfig.DataReferenceConfiguration object at 0x7f0ebc300588>\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
    "\n",
    "# Create a new runconfig object\n",
    "run_amlcompute = RunConfiguration()\n",
    "\n",
    "# Use the cpu_cluster you created above. \n",
    "run_amlcompute.target = gpu_cluster\n",
    "\n",
    "# Enable Docker\n",
    "run_amlcompute.environment.docker.enabled = True\n",
    "\n",
    "# Set Docker base image to the default CPU-based image\n",
    "run_amlcompute.environment.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_amlcompute.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Auto-prepare the Docker image when used for execution (if it is not already prepared)\n",
    "run_amlcompute.auto_prepare_environment = True\n",
    "\n",
    "dr = fds.path('images') # in this example `ds` is the default datastore where you uploaded your MNIST data\n",
    "run_amlcompute.data_references = {dr.data_reference_name: dr.to_config()}\n",
    "print(dr.to_config())\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "run_amlcompute.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = [\n",
    "    '--data-folder',\n",
    "    str(dr),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "\n",
    "# start the training, this will generate a link to the azure portal which will allow monitoring of the training run.\n",
    "\n",
    "\n",
    "src = ScriptRunConfig(source_directory = project_folder, script = 'pytorch_train.py', run_config = run_amlcompute,arguments=script_params)\n",
    "run = exp.submit(src)\n",
    "run.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
